\chapter{Autoregressive Markov Models}
\label{appdx:autoregressive}

In Chapter~\ref{chap:markovnet} Section~\ref{sect:diff-enc}, we introduced a differential encoding network based on an autoregressive Markov model that was A) one-step and B) scalar. In this appendix, we show how we can generalize both the number of steps and the dimension of the Markov model.

Recall the truncated delay domain CSI at the $t$-th timeslot, $\mathbf{H}_{t}\in\mathbb{C}^{R_d \times N_b}$. Rather than the one-step, scalar model $\hat\gamma \in \mathbb R^+$ (see (\ref{eq:diff-estim}) in Section~\ref{sect:diff-enc}), we can derive a $p$-step, multivariate predictor. A $p$-step autoregressive model for $\mathbf{H}_t$ can be written generally as a function of $p$ previous timeslots,
\begin{align*}
	\hat{\mathbf{H}}_t &= f(\mathbf{H}_{t-1}, \mathbf{H}_{t-2}, \dots, \mathbf{H}_{t-p}).
\end{align*}
Given $p$ prior CSI samples, the mean-square optimal predictor $\hat{\mathbf{H}}_t$ is a linear combination of these the prior CSI samples,
\begin{equation}
\mathbf{\hat H}_{t} = \mathbf{H}_{t-1} \mathbf W_{1} + \dots + \mathbf{H}_{t-p} \mathbf W_{p} + \mathbf E_t.
\end{equation}

Where $\mathbf{W}_{i}$ is the coefficient matrix foro the $i$-th timeslot with dimension $\mathbb{C}^{N_b \times N_b}$, and $\mathbf{E}_t$ is the error term at time $t$, which is uncorrelated with the CSI samples (i.e. $\mathbf H_{t-i}^H \mathbf E_t = 0$ for all $i \in [0, \dots, p]$). To solve for the matrices $\mathbf{W}_{t-1}, \dots, \mathbf{W}_{t-p}$, we first pre-multiply by $\mathbf H_{t-i}^H$,
\begin{align}
\mathbf{H}_{t-i}^H\mathbf{\hat H}_{t} &= \mathbf{H}_{t-i}^H\mathbf{H}_{t-1} \mathbf W_{1} + \dots + \mathbf{H}_{t-i}^H\mathbf{H}_{t-p} \mathbf W_{p} + \mathbf{H}_{t-i}^H\mathbf E_t \nonumber \\
                    &= \mathbf{H}_{t-i}^H\mathbf{H}_{t-1} \mathbf W_{1} + \dots + \mathbf{H}_{t-i}^H\mathbf{H}_{t-p} \mathbf W_{p}. \label{eq:var-init}
\end{align}

Denote the correlation matrix 
$\mathbf R_i = \mathbb E [\mathbf H^H_{t-i}\mathbf H_{t}]$. If temporal coherence between timeslots is maintained, then we may assume that CSI matrices arise from a stationary process, implying the following properties:
\begin{enumerate}
  \item $\mathbf R_i = \mathbb E [\mathbf H^H_{t-i}\mathbf H_{t}] = \mathbb E [\mathbf H^H_{t}\mathbf H_{t+i}]$
  \item $\mathbf R_i = \mathbf R^H_{-i}$
\end{enumerate}

With these properties in mind, we take the expectation of (\ref{eq:var-init}), 
resulting in a linear combination of $\mathbf R_i$ matrices,
\begin{align*}
\mathbb E\left[\mathbf{H}_{t-i}^H\mathbf{\hat H}_{t}\right] &= \mathbb E\left[\mathbf{H}_{t-i}^H\mathbf{H}_{t-1} \mathbf W_{1}\right] + \dots + \mathbb E\left[\mathbf{H}_{t-i}^H\mathbf{H}_{t-p} \mathbf W_{p}\right] \\
\mathbf R_{i+1} &= \mathbf{R}_{i} \mathbf W_{1} + \dots + \mathbf{R}_{i-p+1} \mathbf W_{p}. 
\end{align*}
For $p$ CSI samples (i.e., for $i \in [0, 1, \dots, p-1]$), we write a system of $p$ equations, admitting the following,
\begin{align}
  \begin{bmatrix}
    \mathbf R_{1} \\ \mathbf R_{2} \\ \dots \\ \mathbf R_{p} \\
  \end{bmatrix}
  &= 
  \begin{bmatrix}
    \mathbf R_{0} & \mathbf R_1^H & \dots  & \mathbf R_{p-1}^H \\
    \mathbf R_{1} & \mathbf R_0   & \dots  & \mathbf R_{p-2}^H \\
    \vdots      &         & \ddots & \vdots \\
    \mathbf R_{p-1} & \mathbf R_{p-2}   & \dots  & \mathbf R_{0} \\
  \end{bmatrix}
  \begin{bmatrix}
    \mathbf W_{1} \\ \mathbf W_{2} \\ \dots \\ \mathbf W_{p} \\
  \end{bmatrix}. \label{eq:toep}
\end{align}
Finally, we solve for the coefficient matrices by inverting the Toeplitz matrix,
\begin{align}
  \begin{bmatrix}
    \mathbf W_{1} \\ \mathbf W_{2} \\ \dots \\ \mathbf W_{p} \\
  \end{bmatrix}
  &= 
  \begin{bmatrix}
    \mathbf R_{0} & \mathbf R_1^H & \dots  & \mathbf R_{p-1}^H \\
    \mathbf R_{1} & \mathbf R_0   & \dots  & \mathbf R_{p-2}^H \\
    \vdots      &         & \ddots & \vdots \\
    \mathbf R_{p-1} & \mathbf R_{p-2}   & \dots  & \mathbf R_{0} \\
  \end{bmatrix}^{-1}
  \begin{bmatrix}
    \mathbf R_{1} \\ \mathbf R_{2} \\ \dots \\ \mathbf R_{p} \\
  \end{bmatrix}. \label{eq:toep-sol}
\end{align}
Thus, the solution to multi-step, multivariate Markov model is a function of the correlation matrices.

Since the distributions of $\mathbf{H}_i$ are not known, we cannot calculate the expectations in the correlation matrices, $\mathbf R_i$. Instead, we estimate the sample correlation matrices with the training data, i.e.,
\begin{align*}
	\hat{\mathbf{R}}_i = \frac{1}{N_{\text{train}}} \sum_{j}^{N_{\text{train}}} \mathbf{H}_{t-i}(j)^{H}\mathbf{H}_{t}(j),
\end{align*}
where $j$ indexes over the training data. Now, we write the estimators, $\hat{\mathbf{W}}_i$, with respect to the sample correlation matrices,
\begin{align}
  \begin{bmatrix}
    \hat{\mathbf W}_{1} \\ \hat{\mathbf W}_{2} \\ \dots \\ \hat{\mathbf W}_{p} \\
  \end{bmatrix}
  &= 
  \begin{bmatrix}
    \hat{\mathbf R}_{0} & \hat{\mathbf R}_1^H & \dots  & \hat{\mathbf R}_{p-1}^H \\
    \hat{\mathbf R}_{1} & \hat{\mathbf R}_0   & \dots  & \hat{\mathbf R}_{p-2}^H \\
    \vdots      &         & \ddots & \vdots \\
    \hat{\mathbf R}_{p-1} & \hat{\mathbf R}_{p-2}   & \dots  & \hat{\mathbf R}_{0} \\
  \end{bmatrix}^{-1}
  \begin{bmatrix}
    \hat{\mathbf R}_{1} \\ \hat{\mathbf R}_{2} \\ \dots \\ \hat{\mathbf R}_{p} \\
  \end{bmatrix}. \label{eq:toep-sol-est}
\end{align}

If a more simple model is desired, then we can construct scalar estimators for each timeslot. Denote the mean of all elements in the correlation matrix as,
\begin{align*}
  r_i = \mathbb{E}\left[\sum_{k}^{R_d} \sum_{\ell}^{N_b}\mathbf{R}_i^{(k,\ell)}\right]
\end{align*}
where $\mathbf{R}_i^{(k, \ell)}$ is the element from the $k$-th row and $\ell$-th column of $\mathbf{R}_i$. The resulting system of equations for scalar model is given as
\begin{align}
  \begin{bmatrix}
    \gamma_{1} \\ \gamma_{2} \\ \dots \\ \gamma_{p} \\
  \end{bmatrix}
  &= 
  \begin{bmatrix}
    r_{0} & r_1^* & \dots  & r_{p-1}^* \\
    r_{1} & r_0   & \dots  & r_{p-2}^* \\
    \vdots      &         & \ddots & \vdots \\
    r_{p-1} & r_{p-2}   & \dots  & r_{0} \\
  \end{bmatrix}^{-1}
  \begin{bmatrix}
    r_{1} \\ r_{2} \\ \dots \\ r_{p} \\
  \end{bmatrix}. \label{eq:toep-sol-scalar}
\end{align}
Following the same logic as before, we utilize the sample versions of $\hat r_i$,
\begin{align*}
  \hat r_i &= \frac{1}{N_{\text{train}}} \sum_{j}^{N_{\text{train}}} \sum_{k}^{R_d} \sum_{\ell}^{N_b}\hat{\mathbf{R}}_i^{(k,\ell)}(j),
\end{align*}
which admits the system of equations,
\begin{align}
  \begin{bmatrix}
    \hat\gamma_{1} \\ \hat\gamma_{2} \\ \dots \\ \hat\gamma_{p} \\
  \end{bmatrix}
  &= 
  \begin{bmatrix}
    \hat r_{0} & \hat r_1^* & \dots  & \hat r_{p-1}^* \\
    \hat r_{1} & \hat r_0   & \dots  & \hat r_{p-2}^* \\
    \vdots      &         & \ddots & \vdots \\
    \hat r_{p-1} & \hat r_{p-2}   & \dots  & \hat r_{0} \\
  \end{bmatrix}^{-1}
  \begin{bmatrix}
    \hat r_{1} \\ \hat r_{2} \\ \dots \\ \hat r_{p} \\
  \end{bmatrix}. \label{eq:toep-sol-scalar-est}
\end{align}
For the one-step scalar estimator, we can derive equation (\ref{eq:gamma-hat}) from Chapter~\ref{chap:markovnet} of Section~\ref{sect:diff-enc},
\begin{align*}
  \hat{\gamma}_1 &= \frac{\hat{r}_1}{\hat{r}_0} \\
  &= \frac{\text{Trace}(\hat{\mathbf R}_1(k,l))}{\sum_k^{R_d}\sum_l^{N_b}\hat{\mathbf R}_0(k,l)}.
\end{align*}