\chapter{Computational Complexity of Common Layers}
\label{appdx:complexity}
Based on the measures of computational complexity in Chapter~\ref{chap:sph_norm} Section~\ref{sect:dl_overview}, i.e. FLOPs and parameters, we provide the corresponding formulas for common layers and operations used in the networks described in this dissertation. Note that any arithmetic operation (e.g., addition, multiplication) or non-linearity consumes a single FLOP, and certain non-linearities require a single parameter (e.g., the negative slope of a Leaky ReLU).

\section{Matrix Multiplication} \label{appdx:complexity-matmul}

Denote two matrices $\mathbf{A} \in \mathbb{R}^{M \times P}$ and $\mathbf{B} \in \mathbb{R}^{P \times N}$. Matrix multiplication between $\mathbf{A}$ and $\mathbf{B}$ is denoted as
\begin{align*}
	\mathbf{C} &= \mathbf{A}\mathbf{B} \in \mathbb{R}^{M\times N}, \; c_{ij} = \sum_{k=1}^P a_{ik}b_{kj} \; \forall \; i\in [1,\dots,M], i\in [1,\dots,N].
\end{align*}
\textbf{FLOPs}: Each element in $\mathbf{C}$ involves $P$ multiplications and $P-1$ additions, and $\mathbf{C}$ includes $M \times N$ elements. Thus, the amount of FLOPs involved in matrix multiplication is
\begin{align*}
	\text{FLOPs}_{\text{matmul}} &= (2P-1)MN
\end{align*}

\textbf{Parameters}: The number of `parameters' in matrix multiplicatiion depends on which matrix is considered the parameter matrix. If $\mathbf{A}$ is the parameter matrix, then the number of parameters is
\begin{align*}
	P_{\text{matmul}} &= MP.
\end{align*}
Alternatively, if $\mathbf{B}$ is the parameter matrix, then the number of parameters is
\begin{align*}
	P_{\text{matmul}} &= PN.
\end{align*}

\section{Complex Matrix Multiplication}  \label{appdx:complexity-matmul-comp}

Denote two complex matrices $\mathbf{A} \in \mathbb{C}^{M \times P}$ and $\mathbf{B} \in \mathbb{C}^{P \times N}$. Complex matrix multiplication between $\mathbf{A}$ and $\mathbf{B}$ is denoted as
\begin{align*}
	\mathbf{C} &= \mathbf{A}\mathbf{B} \in \mathbb{R}^{M\times N},\\ \; c_{ij} &= \sum_{k=1}^P [\text{Re}(a_{ik})\text{Re}(b_{kj}) - \text{Im}(a_{ik})\text{Im}(b_{kj})] \\ &\phantom{\sum} + j[\text{Re}(a_{ik})\text{Im}(b_{kj}) + \text{Im}(a_{ik})\text{Re}(b_{kj})] \\ &\phantom{\sum} \forall \; i\in [1,\dots,M], i\in [1,\dots,N].
\end{align*}
\textbf{FLOPs}: Each element in $\mathbf{C}$ involves $P$ complex multiplications and $P-1$ complex additions. Complex multiplication involves 4 (real) multiplications and 2 (real) additions, totaling 6 FLOPs total. Complex addition involves 2 (real) additions. Since $\mathbf{C}$ includes $M \times N$ complex elements, the amount of FLOPs involved in matrix multiplication is
\begin{align*}
	\text{FLOPs}_{\text{matmul}} &= (8P-2)MN % (6P + 2(P-1))MN
\end{align*}

\textbf{Parameters}: The number of parameters in complex matrix multiplication is identical to real matrix multiplication with a factor of 2. If $\mathbf{A}$ is the parameter matrix, then the number of parameters is
\begin{align*}
	P_{\text{matmul}} &= 2MP.
\end{align*}
Alternatively, if $\mathbf{B}$ is the parameter matrix, then the number of parameters is
\begin{align*}
	P_{\text{matmul}} &= 2PN.
\end{align*}

\section{Linear Layer} \label{appdx:complexity-linear}
Denote parameter matrix $\mathbf{A}\in\mathbb{R}^{M\times N}$ and the input vector $\mathbf{b}\in\mathbb{R}^{N}$. The output of a linear layer, $\mathbf{o}\in\mathbb{R}^M$, is given as
\begin{align*}
	\mathbf{o} &= \mathbf{A}\mathbf{b}.
\end{align*}
If the linear layer contains a bias term, then an additional column $\mathbf{a}_0$ is appended to the end of the matrix $\mathbf{A}$ as
\begin{align*}
	\mathbf{A}_{\text{bias}} &= \begin{bmatrix}\mathbf{A} & \mathbf{a}_0\end{bmatrix},
\end{align*}
and correspondingly, the input vector is padded with a single one,
\begin{align*}
	\mathbf{b}_{\text{bias}} &= \begin{bmatrix}\mathbf{b} & 1\end{bmatrix}.
\end{align*}

\textbf{FLOPs}: A linear layer has the same number of FLOPs as a matrix-vector multiplication, i.e.
\begin{align*}
	\text{FLOPs}_{\text{linear}} &= M(N-1)
\end{align*}

\textbf{Parameters}: The size of the matrix $\mathbf{A}$ determines the number of parameters in the layer, i.e.,
\begin{align*}
	P_{\text{linear}} &= MN
\end{align*}

\section{Convolutional Layer}

In a convolutional layer, the complexity is driven by the number of kernels ($N_k$), the height/width of kernel ($H_k/W_k$), and the channels/height/width of the input data ($C/H/W$). Denote the input data as $\mathbf I\in\mathbb{R}^{C\times H\times W}$, the kernel as $\mathbf{K}\in\mathbb{R}^{N_k\times H_k\times W_k}$, and the output image as $\mathbf O\in\mathbb{R}^{N_k\times H\times W}$. The convolution operation (with no bias term) assigns a value to each element of the output using the following equation,
\begin{align}
	\mathbf{O}(i,j,k) &= \sum_{c=1}^{C}\sum_{h=1}^{H_k}\sum_{w=1}^{W_k} K(i,j+h,k+w)I\left(c,j+h-\left\lfloor\frac{H}{2}\right\rfloor,k+w-\left\lfloor\frac{W}{2}\right\rfloor\right), \label{eq:2d-conv}
\end{align}
where the $i,j,k$ index over the output channels, rows, and columns (respectively). On the right-hand side of (\ref{eq:2d-conv}), the first summation $\left(\sum_{c=1}^C\right)$ sums over the channels of the input image while the following two summations $\left(\sum_{h=1}^{H_k}/\sum_{w=1}^{W_k}\right)$ sum over the height/width of the kernel. Whenever the height or width indices of $\mathbf{I}$ are out of the range $[1,\dots,H]$ or $[1,\dots,W]$, the elements involved in the convolution are determined by the padding (e.g., zero padding, reflected padding, etc.).

\textbf{FLOPs}: The number of FLOPs in a convolutional layer is given by the following formula,
\begin{align*}
	\text{FLOPs}_{\text{conv}} &= C\times H \times W \times N_k \times H_k \times W_k
\end{align*}

\textbf{Parameters}: The number of parameters in a convolutional layer is given by the following formula,
\begin{align*}
	P_{\text{conv}} &= N_k\times H_k\times W_k
\end{align*}

\section{Soft Threshold Function}

The soft threshold function used in ISTANet+ \cite{ref:zhang2018ista} is given in (\ref{eq:soft}), replicated below as,
\begin{align*}
    \text{soft}(x, \theta) &= \text{sign}(x)\text{ReLU}(|x|-\theta),
\end{align*}
where $\theta$ is the threshold function.

\textbf{FLOPs}: We consider a single soft threshold to consume one FLOP.

\textbf{Parameters}: The soft threshold function requires one parameter to be stored, $\theta$.