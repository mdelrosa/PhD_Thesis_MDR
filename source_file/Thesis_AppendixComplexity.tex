\chapter{Computational Complexity of Common Layers}
\label{appdx:complexity}
Based on the measures of computational complexity in Chapter~\ref{chap:sph_norm} Section~\ref{sect:dl_overview}, i.e. FLOPs and parameters, we provide the corresponding formulas for common layers and operations used in the networks described in this thesis. Note that any arithmetic operation (e.g., addition, multiplication) or non-linearity consumes a single FLOP, and certain non-linearities require a single parameter (e.g., the negative slope of a Leaky ReLU).

\section{Linear Layer} \label{appdx:complexity-linear}
Denote parameter matrix $\mathbf{A}\in\mathbb{R}^{M\times N}$ and the input vector $\mathbf{b}\in\mathbb{R}^{N}$. The output of a linear layer, $\mathbf{o}\in\mathbb{R}^M$, is given as
\begin{align*}
	\mathbf{o} &= \mathbf{A}\mathbf{b}.
\end{align*}
If the linear layer contains a bias term, then an additional column $\mathbf{a}_0$ is appended to the end of the matrix $\mathbf{A}$ as
\begin{align*}
	\mathbf{A}_{\text{bias}} &= \begin{bmatrix}\mathbf{A} & \mathbf{a}_0\end{bmatrix},
\end{align*}
and correspondingly, the input vector is padded with a single one,
\begin{align*}
	\mathbf{b}_{\text{bias}} &= \begin{bmatrix}\mathbf{b} & 1\end{bmatrix}.
\end{align*}

\textbf{FLOPs}: A linear layer has the same number of FLOPs as a matrix-vector multiplication, i.e.
\begin{align*}
	\text{FLOPs}_{\text{linear}} &= M(N-1)
\end{align*}

\textbf{Parameters}: The size of the matrix $\mathbf{A}$ determines the number of parameters in the layer, i.e.,
\begin{align*}
	P_{\text{linear}} &= MN
\end{align*}

\section{Convolutional Layer}

In a convolutional layer, the complexity is driven by the number of kernels ($N_k$), the height/width of kernel ($H_k/W_k$), and the channels/height/width of the input data ($C/H/W$). Denote the input data as $\mathbf I\in\mathbb{R}^{C\times H\times W}$, the kernel as $\mathbf{K}\in\mathbb{R}^{N_k\times H_k\times W_k}$, and the output image as $\mathbf O\in\mathbb{R}^{N_k\times H\times W}$. The convolution operation assigns a value to each element of the output using the following equation,
\begin{align}
	\mathbf{O}(i,j,k) &= \sum_{c=1}^{C}\sum_{h=1}^{H_k}\sum_{w=1}^{W_k} K(i,j+h,k+w)I\left(c,j+h-\left\lfloor\frac{H}{2}\right\rfloor,k+w-\left\lfloor\frac{W}{2}\right\rfloor\right)
\end{align}
Whenever the height or width indices of $\mathbf{I}$ are out of the range $[1,\dots,H]$ or $[1,\dots,W]$, the elements involved in the convolution are determined by the padding (e.g., zero padding, reflected padding, etc.).

\textbf{FLOPs}: The number of FLOPs in a convolutional layer is given by the following formula,
\begin{align*}
	\text{FLOPs}_{\text{conv}} &= C\times H \times W \times N_k \times H_k \times W_k
\end{align*}

\textbf{Parameters}: The number of parameters in a convolutional layer is given by the following formula,
\begin{align*}
	P_{\text{conv}} &= N_k\times H_k\times W_k
\end{align*}

\section{Soft Threshold Function}

The soft threshold function used in ISTANet+ \cite{ref:zhang2018ista} is given in (\ref{eq:soft}), replicated below as,
\begin{align*}
    \text{soft}(x, \theta) &= \text{sign}(x)\text{ReLU}(|x|-\theta),
\end{align*}
where $\theta$ is the threshold function.

\textbf{FLOPs}: We consider a single soft threshold to consume one FLOP.

\textbf{Parameters}: The soft threshold function requires one parameter to be stored, $\theta$.